import streamlit as st
import os
import pandas as pd
from datetime import datetime
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma
from langchain.prompts import PromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from langchain_community.llms import LlamaCpp

# --- Configuraci칩n y Carga ---

# Constantes
DB_PATH = "vectorstores/db/"
# Ruta al modelo GGUF local. Se asume que est치 en la carpeta del otro proyecto.
MODEL_PATH = "C:/Users/jsanc/Proyectos IA/Proyecto_MAPEAR_PRO/backend/models/models--bartowski--Meta-Llama-3.1-8B-Instruct-GGUF/snapshots/bf5b95e96dac0462e2a09145ec66cae9a3f12067/Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf"

# Template del prompt para citas en l칤nea
PROMPT_TEMPLATE = """### INSTRUCCIONES:
Tu tarea es actuar como un asistente experto en an치lisis de evidencia cient칤fica y responder la pregunta del usuario.
Te basar치s 칔NICA Y EXCLUSIVAMENTE en el contexto que te proporciono, el cual consiste en varias fuentes numeradas.

Reglas estrictas:
1.  Sintetiza la informaci칩n de las fuentes para construir una respuesta coherente y fluida.
2.  **CR칈TICO**: Despu칠s de cada oraci칩n o afirmaci칩n que extraigas de una fuente, DEBES a침adir la cita correspondiente. Por ejemplo: "La eficacia del tratamiento fue del 80% [1]."
3.  Si una misma oraci칩n combina informaci칩n de m칰ltiples fuentes, cita todas las relevantes. Por ejemplo: "El estudio incluy칩 pacientes de Argentina [1] y Chile [2]."
4.  NO inventes informaci칩n. Si la respuesta no se encuentra en las fuentes proporcionadas, responde exactamente con: "La informaci칩n solicitada no se encuentra en los documentos disponibles."
5.  No incluyas en tu respuesta los nombres de los documentos, solo los n칰meros de cita.

### CONTEXTO:
{context}

### PREGUNTA:
{question}

### RESPUESTA CUMPLIENDO LAS REGLAS:
"""

@st.cache_resource
def load_resources():
    """Carga y cachea los recursos pesados (modelos y BD) para evitar recargarlos.
    
    Returns:
        Tuple: Una tupla con el objeto de la base de datos, el objeto del LLM y un 
               mensaje de error (si lo hay).
    """
    # Validar que el archivo del modelo exista antes de intentar cargarlo.
    if not os.path.exists(MODEL_PATH):
        return None, None, f"Error: No se encuentra el archivo del modelo en la ruta: {MODEL_PATH}"

    # Cargar el modelo de embeddings para la b칰squeda de similitud.
    embeddings = HuggingFaceEmbeddings(
        model_name='sentence-transformers/all-MiniLM-L6-v2',
        model_kwargs={'device': 'cpu'}
    )
    # Cargar la base de datos vectorial persistida en disco.
    db = Chroma(persist_directory=DB_PATH, embedding_function=embeddings)
    
    # Cargar el LLM local usando LlamaCpp. 
    # n_gpu_layers=-1 intenta descargar todas las capas a la GPU si est치 disponible.
    llm = LlamaCpp(
        model_path=MODEL_PATH,
        n_ctx=4096,       # Longitud del contexto
        n_gpu_layers=-1,  # Aceleraci칩n por GPU
        verbose=False,
        temperature=0.1
    )
    
    return db, llm, None

def get_rag_chain(llm, retriever):
    """Crea y devuelve la cadena RAG completa."""
    prompt = PromptTemplate(template=PROMPT_TEMPLATE, input_variables=["context", "question"])
    
    rag_chain = (
        prompt
        | llm
        | StrOutputParser()
    )
    return rag_chain

def log_feedback(rating: str):
    """Registra el feedback del usuario en un archivo CSV."""
    feedback_data = st.session_state.last_response_data
    feedback_data['rating'] = rating
    
    df = pd.DataFrame([feedback_data])
    
    try:
        if os.path.exists("feedback.csv"):
            df.to_csv("feedback.csv", mode='a', header=False, index=False, encoding='utf-8')
        else:
            df.to_csv("feedback.csv", mode='w', header=True, index=False, encoding='utf-8')
    except Exception as e:
        st.error(f"Error al guardar el feedback: {e}")

def main():
    st.title("游뱄 Consulta de Evidencia en Tecnolog칤as Sanitarias (Local)")
    st.caption("Este prototipo usa el modelo Llama-3.1-8B cargado localmente.")
    st.caption("Autor: Juli치n S치nchez Viamonte")

    db, llm, error_message = load_resources()

    if error_message:
        st.error(error_message)
        st.info("Aseg칰rate de que la ruta al modelo .gguf es correcta en el script app.py.")
        return

    # --- Interfaz de Usuario ---
    query_text = st.text_input("Escribe tu pregunta sobre los documentos:", placeholder="Ej: 쮺u치l es la eficacia de la IA en la supervisi칩n cl칤nica?")

    k_value = st.slider(
        label="N칰mero de fragmentos a recuperar (k):",
        min_value=1,
        max_value=10,
        value=3,
        help="Controla cu치ntos fragmentos de texto se usan como contexto para la respuesta. Un n칰mero m치s alto puede dar m치s informaci칩n pero tambi칠n m치s 'ruido'."
    )

    # --- L칩gica de la Aplicaci칩n ---
    if 'feedback_given' not in st.session_state:
        st.session_state.feedback_given = False
    if 'last_response_data' not in st.session_state:
        st.session_state.last_response_data = None

    if st.button("游댌 Buscar") and query_text:
        st.session_state.feedback_given = False
        st.session_state.last_response_data = None
        with st.spinner("Buscando en los documentos y generando respuesta con el modelo local..."):
            retriever = db.as_retriever(search_kwargs={'k': k_value})
            rag_chain = get_rag_chain(llm, retriever) # Usar la funci칩n refactorizada

            try:
                retrieved_docs = retriever.invoke(query_text)
                formatted_context = ""
                for i, doc in enumerate(retrieved_docs):
                    formatted_context += f"### Fuente [{i+1}]\n"
                    formatted_context += f"Documento: {doc.metadata.get('source', 'N/A').replace('documentos\\', '')}\n"
                    formatted_context += f"Contenido: {doc.page_content}\n\n"

                response = rag_chain.invoke({"context": formatted_context, "question": query_text})
                
                st.session_state.last_response_data = {
                    "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                    "question": query_text,
                    "answer": response,
                    "context": formatted_context
                }
            except Exception as e:
                st.error(f"Ocurri칩 un error al generar la respuesta: {e}")
                st.session_state.last_response_data = None

    # --- Mostrar Respuesta y Sistema de Feedback ---
    if st.session_state.last_response_data:
        st.subheader("Respuesta Generada")
        st.markdown(st.session_state.last_response_data["answer"])

        with st.expander("Ver fuentes utilizadas"):
            st.markdown(st.session_state.last_response_data["context"])
        
        st.write("\n---")
        st.write("**쮽ue 칰til esta respuesta?**")

        if not st.session_state.feedback_given:
            col1, col2, _ = st.columns([1, 1, 5])
            with col1:
                if st.button("游녨"):
                    log_feedback(rating="游녨 Buena respuesta")
                    st.session_state.feedback_given = True
                    st.rerun()
            with col2:
                if st.button("游녩"):
                    log_feedback(rating="游녩 Mala respuesta")
                    st.session_state.feedback_given = True
                    st.rerun()
        else:
            st.success("춰Gracias por tu feedback!")

if __name__ == '__main__':
    main()
